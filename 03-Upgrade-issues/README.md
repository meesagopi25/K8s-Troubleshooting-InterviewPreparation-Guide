Below is a **clear, realistic explanation WITH a full working example** that shows:

* A **rolling update failing**
* The **new pods failing readiness**
* Kubernetes **pausing the rollout**
* Automatic **rollback protection**
* How to debug and fix it

Perfect to add to interviews or a README.

---

# ‚úÖ **Why a Rolling Update Fails When New Pods Fail Readiness**

During a rolling update:

1. Kubernetes creates a **new pod (v2)**
2. It waits for the pod to become **Ready**
3. If readiness fails too many times:

   * Kubernetes **stops the rollout**
   * It keeps the old version running
   * It marks the rollout as **‚Äúprogress deadline exceeded‚Äù**

This protects you from deploying a defective version.

---

# üö® **Example: Rolling Update Fails Due to Wrong Readiness Probe**

Below is a Deployment that works in v1 but fails in v2.

---

# 1Ô∏è‚É£ **Working Version (v1): readiness probe correct**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
        version: v1
    spec:
      containers:
      - name: app
        image: nginx
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 3
          periodSeconds: 5
```

Apply:

```bash
kubectl apply -f web-v1.yaml
```

Pods become ready.

---

# 2Ô∏è‚É£ **Update to v2 ‚Äî readiness probe broken**

Example: the probe points to a **non-existing path**, so readiness fails.

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
        version: v2
    spec:
      containers:
      - name: app
        image: nginx
        readinessProbe:
          httpGet:
            path: /healthz   # ‚ùå Nginx does NOT serve this path ‚Üí always 404 ‚Üí readiness FAILS
            port: 80
          initialDelaySeconds: 3
          periodSeconds: 5
```

Apply the update:

```bash
kubectl apply -f web-v2.yaml
```

---

# 3Ô∏è‚É£ **What Happens During the Failed Rolling Update**

Kubernetes starts upgrading:

### ‚úî Step 1 ‚Äî It creates 1 surge pod:

```
web-xxx-v2   Pending ‚Üí Running ‚Üí NOT READY
```

### ‚ùå Readiness probe repeatedly fails:

```
HTTP probe failed with statuscode: 404
```

### ‚úî Step 2 ‚Äî Kubernetes **does NOT kill old pods**

Because:

* `maxUnavailable = 0`
* A new pod is not ready yet

### ‚ùå Step 3 ‚Äî Rollout eventually fails with timeout:

```
kubectl rollout status deploy/web
‚Üí error: deployment "web" exceeded its progress deadline
```

This means:

> New version is bad ‚Üí rollback protection activated.

---

# 4Ô∏è‚É£ **Verify the failing readiness probe**

Use:

```bash
kubectl describe pod <v2-pod>
```

You will see:

```
Warning  Unhealthy  Readiness probe failed: HTTP probe failed with statuscode: 404
```

---

# 5Ô∏è‚É£ **Rolling update is paused ‚Äî old version still serving traffic**

Check replica sets:

```bash
kubectl get rs
```

You will see:

* v1 ReplicaSet: 2 pods (still running)
* v2 ReplicaSet: 1 pod (not ready)

Traffic still goes to the healthy v1 pods ‚Üí **zero downtime**.

---

# 6Ô∏è‚É£ **Fix the readiness probe**

Correct readiness probe:

```yaml
readinessProbe:
  httpGet:
    path: /
    port: 80
  initialDelaySeconds: 1
  periodSeconds: 3
```

Apply and rollout resumes:

```bash
kubectl apply -f web-v2-fixed.yaml
kubectl rollout status deploy/web
```

---

# üéØ **OTHER COMMON REAL-WORLD CAUSES OF ROLLING UPDATE FAILURE**

| Cause                                                        | Example                               |
| ------------------------------------------------------------ | ------------------------------------- |
| Wrong readiness probe path                                   | `/healthz` instead of `/`             |
| Wrong probe port                                             | App listens on 8080, probe checks 80  |
| App starts but waits for DB                                  | Readiness fails until DB accessible   |
| App has a bug in v2                                          | CrashLoopBackOff or readiness failure |
| Misconfigured configmap/secret                               | Wrong env vars cause boot failure     |
| Using `maxUnavailable: 0` with insufficient cluster capacity | New pods can‚Äôt be scheduled           |
| DB migration breaks backward compatibility                   | v2 works but v1 fails ‚Üí rollback loop |

---

# üß† **Why Kubernetes Rolls Back?**

Because of the deployment rule:

> A rolling update cannot continue until the new pod is **Ready**.

If readiness never succeeds ‚Üí rollout freezes ‚Üí protects production.

---

# üß™ READY-TO-COPY SUMMARY FOR README

> During a rolling update, Kubernetes replaces old pods with new ones gradually.
> If the new pods fail their readiness probe (bad path/port, missing dependency, config error), Kubernetes stops the rollout and keeps the old version running.
> With `maxUnavailable: 0`, Kubernetes ensures **zero downtime** but also means a bad release will **never proceed**, effectively preventing production outages.

---

Absolutely ‚Äî here is a **clean, complete, practical, step-by-step guide** to perform and test a **Zero-Downtime Deployment** in Kubernetes.
This is the *exact* version you should add to your README.

Everything below *works on any Kubernetes cluster*: Minikube, KIND, OpenShift, or cloud.

---

# ‚úÖ **Zero-Downtime Deployment ‚Äì Full Practical Lab**

This lab demonstrates:

* Rolling updates with **zero downtime**
* Readiness-driven traffic switching
* How Kubernetes prevents bad deployments
* How to test service continuity during an upgrade

---

# üß± **1. Create Deployment v1 (initial stable version)**

Save as **web-v1.yaml**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zero-web
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0      # Ensure no downtime (at least 2 pods always running)
      maxSurge: 1            # Add 1 extra pod temporarily during rollout
  selector:
    matchLabels:
      app: zero-web
  template:
    metadata:
      labels:
        app: zero-web
        version: v1
    spec:
      containers:
      - name: app
        image: nginx
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 2
          periodSeconds: 3
```

Apply:

```bash
kubectl apply -f web-v1.yaml
kubectl rollout status deploy/zero-web
```

Service (save as svc.yaml):

```yaml
apiVersion: v1
kind: Service
metadata:
  name: zero-web-svc
spec:
  selector:
    app: zero-web
  ports:
  - port: 80
    targetPort: 80
```

Apply:

```bash
kubectl apply -f svc.yaml
```

---

# üß™ **2. Start a continuous traffic test (to verify zero downtime)**

Run a test pod:

```bash
kubectl run tester --rm -it --image=busybox -n default -- sh
```

Inside the shell:

```sh
while true; do wget -qO- http://zero-web-svc; echo ""; sleep 1; done
```

You should see:

```
<!DOCTYPE html> <html> ... Welcome to nginx! ...
```

Every second.

Keep this **running during the rollout**.

---

# üöÄ **3. Deploy Version v2 (upgrade without downtime)**

Let's upgrade nginx ‚Üí Apache httpd (or you can modify HTML).
Save as **web-v2.yaml**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zero-web
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  selector:
    matchLabels:
      app: zero-web
  template:
    metadata:
      labels:
        app: zero-web
        version: v2
    spec:
      containers:
      - name: app
        image: httpd:2.4
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 2
          periodSeconds: 3
```

Apply:

```bash
kubectl apply -f web-v2.yaml
kubectl rollout status deploy/zero-web
```

### ‚úî Expected:

* Tester loop NEVER stops printing output.
* Traffic seamlessly shifts to v2.
* No errors, no downtime.

---

# üìå **4. Verify Pods During Rollout**

```bash
kubectl get pods -o wide -w
```

You will see:

1. New v2 pod created ‚Üí becomes Ready
2. Old v1 pod terminated
3. Repeat for 2nd pod

Zero downtime is guaranteed because:

* `maxUnavailable=0` keeps old pods serving until new pods are fully ready.
* Readiness probe ensures traffic only goes to healthy pods.

---

# ‚ùå **5. (Optional) Test a Broken Update ‚Äî Rollout Fail Example**

Use a broken readinessProbe:

```yaml
readinessProbe:
  httpGet:
    path: /healthz   # <-- WRONG PATH
    port: 80
```

Apply:

```bash
kubectl apply -f web-v3-broken.yaml
kubectl rollout status deploy/zero-web
```

### Expected:

* New pods stay `0/1 Ready`
* Existing pods continue serving
* Rollout hangs ‚Üí **no downtime**
* Eventually you see:

```
deployment "zero-web" exceeded its progress deadline
```

This shows **Kubernetes protects production** with zero downtime.

---

# üîß **6. Fix the rollout and resume**

Fix the readinessProbe path:

```yaml
readinessProbe:
  httpGet:
    path: /
    port: 80
```

Apply again:

```bash
kubectl apply -f web-v3-fixed.yaml
kubectl rollout status deploy/zero-web
```

Traffic continues uninterrupted.

---

# üéâ **Summary ‚Äì What You Achieved**

You have now tested:

### ‚úî Zero-downtime rolling updates

### ‚úî Readiness gates

### ‚úî How Kubernetes waits before switching traffic

### ‚úî Surge/Unavailable strategy behavior

### ‚úî Rollback protection

### ‚úî Continuous traffic testing

### ‚úî Handling a broken release safely

This is exactly how real production-grade deployments are validated.

---

# Want to go deeper?

I can provide:

* Blue/Green deployment example
* Canary deployment example
* Weighted traffic split example
* A rollback demo
* A full README documentation section

Just tell me!

